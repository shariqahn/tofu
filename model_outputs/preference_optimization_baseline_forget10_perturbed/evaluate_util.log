Loading checkpoint from locuslab/llama2-7b_idk_1e-05_forget10
Downloading shards:   0%|                                                                              | 0/6 [00:00<?, ?it/s]Downloading shards:  17%|███████████▌                                                         | 1/6 [01:55<09:36, 115.24s/it]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:651: UserWarning: Not enough free disk space to download the file. The expected file size is: 4857.21 MB. The target location /root/.cache/huggingface/hub/models--locuslab--llama2-7b_idk_1e-05_forget10/blobs only has 2503.79 MB free disk space.
  warnings.warn(
Downloading shards:  17%|███████████▌                                                         | 1/6 [02:54<14:34, 174.82s/it]
[Errno 28] No space left on device
Loading checkpoint from locuslab/llama2-7b_idk_1e-05_forget10
Downloading shards:   0%|                                                                              | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:651: UserWarning: Not enough free disk space to download the file. The expected file size is: 4857.21 MB. The target location /root/.cache/huggingface/hub/models--locuslab--llama2-7b_idk_1e-05_forget10/blobs only has 3.85 MB free disk space.
  warnings.warn(
Downloading shards:  17%|███████████▋                                                          | 1/6 [00:00<00:02,  1.84it/s]
[Errno 28] No space left on device
Loading checkpoint from locuslab/llama2-7b_idk_1e-05_forget10
Downloading shards:   0%|                                                                              | 0/6 [00:00<?, ?it/s]Downloading shards:  17%|███████████▋                                                          | 1/6 [00:00<00:02,  2.13it/s]
[Errno 28] No space left on device
Error: could not load model
Error executing job with overrides: ['model_family=llama2-7b', 'split=forget10_perturbed', 'model_path=locuslab/llama2-7b_idk_1e-05_forget10', 'save_root=./model_outputs/preference_optimization_baseline_forget10_perturbed']
Traceback (most recent call last):
  File "/tofu/evaluate_util.py", line 444, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/tofu/evaluate_util.py", line 318, in main
    model = model.eval()
AttributeError: 'NoneType' object has no attribute 'eval'
[2025-02-01 06:00:17,689] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2231) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
evaluate_util.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-01_06:00:17
  host      : 13235e55b5ac
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2231)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
